{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RM-Gallery Workflow Demonstration\n",
    "\n",
    "This notebook demonstrates the complete workflow of RM-Gallery, from data input to final analysis results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "import asyncio\n",
    "import os\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# Load environment variables from .env file\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from rm_gallery.core.graders.base_grader import BaseGrader, GraderMode, GraderResult, GraderScore\n",
    "from rm_gallery.core.graders.llm_grader import LLMGrader\n",
    "from rm_gallery.core.models.openai_chat_model import OpenAIChatModel\n",
    "from rm_gallery.core.models.schema.template import PromptTemplate\n",
    "from rm_gallery.core.models.schema.message import ChatMessage\n",
    "from rm_gallery.core.runner.grading_runner import GradingRunner, GraderConfig\n",
    "from rm_gallery.core.analyzer.base_analyzer import BaseAnalyzer, BaseAnalysis\n",
    "from rm_gallery.core.analyzer.accuracy_analyzer import AccuracyAnalyzer\n",
    "from rm_gallery.core.analyzer.weighted_average_analyzer import WeightedAverageAnalyzer\n",
    "from rm_gallery.core.models.base_chat_model import BaseChatModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define Custom Graders\n",
    "\n",
    "First, we'll define some simple custom graders for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AccuracyGrader(BaseGrader):\n",
    "    \"\"\"A simple accuracy grader that checks if answer matches expected value.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            name=\"accuracy_grader\",\n",
    "            mode=GraderMode.POINTWISE,\n",
    "            description=\"Evaluates accuracy of answers\"\n",
    "        )\n",
    "    \n",
    "    async def aevaluate(self, **kwargs) -> GraderResult:\n",
    "        answer = kwargs.get(\"answer\", \"\")\n",
    "        expected = kwargs.get(\"expected\", None)\n",
    "        \n",
    "        if expected is not None:\n",
    "            score = 1.0 if str(answer) == str(expected) else 0.0\n",
    "            reason = \"Correct answer\" if score == 1.0 else \"Incorrect answer\"\n",
    "        else:\n",
    "            score = 0.5  # Default score when no expected value\n",
    "            reason = \"No expected value provided\"\n",
    "            \n",
    "        return GraderScore(\n",
    "            name=self.name,\n",
    "            score=score,\n",
    "            reason=reason\n",
    "        )\n",
    "\n",
    "class RelevanceGrader(BaseGrader):\n",
    "    \"\"\"A simple relevance grader that gives a relevance score based on answer length.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            name=\"relevance_grader\",\n",
    "            mode=GraderMode.POINTWISE,\n",
    "            description=\"Evaluates relevance of answers based on length\"\n",
    "        )\n",
    "    \n",
    "    async def aevaluate(self, **kwargs) -> GraderResult:\n",
    "        answer = kwargs.get(\"answer\", \"\")\n",
    "        query = kwargs.get(\"query\", \"\")\n",
    "        \n",
    "        # Simple relevance based on answer length and query length\n",
    "        answer_length = len(str(answer))\n",
    "        query_length = len(str(query))\n",
    "        \n",
    "        # Normalize score between 0 and 1\n",
    "        if query_length > 0:\n",
    "            score = min(1.0, answer_length / query_length)\n",
    "        else:\n",
    "            score = 0.5\n",
    "            \n",
    "        reason = f\"Answer length: {answer_length}, Query length: {query_length}\"\n",
    "        \n",
    "        return GraderScore(\n",
    "            name=self.name,\n",
    "            score=score,\n",
    "            reason=reason\n",
    "        )\n",
    "    \n",
    "\n",
    "template = PromptTemplate(\n",
    "    messages=[\n",
    "        ChatMessage(\n",
    "            role=\"system\",\n",
    "            content=\"You are an expert evaluator of AI assistant responses. \"\n",
    "                    \"Rate the helpfulness of the answer on a scale from 0 to 1, \"\n",
    "                    \"where 0 is completely unhelpful and 1 is perfectly helpful. \"\n",
    "        ),\n",
    "        ChatMessage(\n",
    "            role=\"user\",\n",
    "            content=\"\"\"Question: {query}\\nAnswer: {answer}\\n\\n\n",
    "please rate the helpfulness of the answer on a scale from 0 to 1.\n",
    "Response Json Format:\n",
    "```json\n",
    "{{\n",
    "    \"score\": 0.8,\n",
    "    \"reason\": \"Explanation of the score\"\n",
    "}}\n",
    "```\n",
    "\"\"\"\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Custom LLMGrader implementation\n",
    "class HelpfulnessGrader(LLMGrader):\n",
    "    \"\"\"Custom LLM-based grader for evaluating helpfulness using OpenAI API.\"\"\"\n",
    "\n",
    "    def __init__(self, model: BaseChatModel):\n",
    "        super().__init__(\n",
    "            name=\"helpfulness_grader\",\n",
    "            mode=GraderMode.POINTWISE,\n",
    "            description=\"Evaluates helpfulness of answers using LLM as a judge\",\n",
    "            model=model,\n",
    "            template=template,\n",
    "        )\n",
    "    \n",
    "    async def aevaluate(self, query: str, answer: str, **kwargs) -> GraderResult:\n",
    "        return await super().aevaluate(query=query, answer=answer, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare Input Data\n",
    "\n",
    "Let's prepare some sample data for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data:\n",
      "  Sample 1: {'query': 'What is the capital of France?', 'answer': 'Paris', 'expected': 'Paris', 'label': 1.0}\n",
      "  Sample 2: {'query': 'What is 2+2?', 'answer': '5', 'expected': '4', 'label': 0.0}\n",
      "  Sample 3: {'query': 'Who wrote Romeo and Juliet?', 'answer': 'William Shakespeare', 'expected': 'William Shakespeare', 'label': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# Sample data for evaluation\n",
    "data = [\n",
    "    {\n",
    "        \"query\": \"What is the capital of France?\",\n",
    "        \"answer\": \"Paris\",\n",
    "        \"expected\": \"Paris\",\n",
    "        \"label\": 1.0,\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What is 2+2?\",\n",
    "        \"answer\": \"5\",\n",
    "        \"expected\": \"4\",\n",
    "        \"label\": 0.0,\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Who wrote Romeo and Juliet?\",\n",
    "        \"answer\": \"William Shakespeare\",\n",
    "        \"expected\": \"William Shakespeare\",\n",
    "        \"label\": 1.0,\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"Input data:\")\n",
    "for i, item in enumerate(data):\n",
    "    print(f\"  Sample {i+1}: {item}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configure and Run Graders\n",
    "\n",
    "Now we'll set up the graders and run the evaluation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results:\n",
      "  Sample 1:\n",
      "    accuracy_grader: score=1.0, reason='Correct answer'\n",
      "    relevance_grader: score=0.16666666666666666, reason='Answer length: 5, Query length: 30'\n",
      "    helpfulness_grader: score=1.0, reason='The answer is correct and directly addresses the question without any unnecessary information.'\n",
      "  Sample 2:\n",
      "    accuracy_grader: score=0.0, reason='Incorrect answer'\n",
      "    relevance_grader: score=0.08333333333333333, reason='Answer length: 1, Query length: 12'\n",
      "    helpfulness_grader: score=0.0, reason='The answer is incorrect. 2+2 equals 4, not 5. Providing an incorrect answer to a simple arithmetic question is unhelpful.'\n",
      "  Sample 3:\n",
      "    accuracy_grader: score=1.0, reason='Correct answer'\n",
      "    relevance_grader: score=0.7037037037037037, reason='Answer length: 19, Query length: 27'\n",
      "    helpfulness_grader: score=1.0, reason='The answer is completely correct and directly addresses the question. William Shakespeare is indeed the author of Romeo and Juliet, and the response is concise and accurate.'\n"
     ]
    }
   ],
   "source": [
    "# Create grader configurations\n",
    "model = OpenAIChatModel(model=\"qwen3-32b\")\n",
    "\n",
    "\n",
    "grader_configs = [\n",
    "    GraderConfig(grader=AccuracyGrader()),\n",
    "    GraderConfig(grader=RelevanceGrader()),\n",
    "    GraderConfig(grader=HelpfulnessGrader(model=model))\n",
    "]\n",
    "\n",
    "# Create the grading runner\n",
    "runner = GradingRunner(grader_configs=grader_configs, max_concurrency=5)\n",
    "\n",
    "# Run the evaluation\n",
    "results = await runner.arun(data)\n",
    "\n",
    "print(\"Evaluation results:\")\n",
    "for i, sample_results in enumerate(results):\n",
    "    print(f\"  Sample {i+1}:\")\n",
    "    for grader_name, result in sample_results.items():\n",
    "        score = getattr(result, \"score\", 0.0)\n",
    "        print(f\"    {grader_name}: score={score}, reason='{result.reason}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Analyze Results\n",
    "\n",
    "Finally, let's analyze the results using different analyzers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis results:\n",
      "  Accuracy Analysis: name='Accuracy Analysis' metadata={'explanation': 'Correctly predicted 3 out of 3 samples (100.00% accuracy)'} accuracy=1.0\n",
      "  Weighted Average Analysis: name='weighted_average' metadata={} weighted_results=[{'weighted_result': GraderScore(name='weighted_average', reason='Weighted average score calculated from 3 evaluators. accuracy_grader: 1.0 (weight: 0.5); relevance_grader: 0.16666666666666666 (weight: 0.3); helpfulness_grader: 1.0 (weight: 0.2)', metadata={'weights': {'accuracy_grader': 0.5, 'relevance_grader': 0.3, 'helpfulness_grader': 0.2}, 'component_scores': {'accuracy_grader': 1.0, 'relevance_grader': 0.16666666666666666, 'helpfulness_grader': 1.0}}, score=0.75)}, {'weighted_result': GraderScore(name='weighted_average', reason='Weighted average score calculated from 3 evaluators. accuracy_grader: 0.0 (weight: 0.5); relevance_grader: 0.08333333333333333 (weight: 0.3); helpfulness_grader: 0.0 (weight: 0.2)', metadata={'weights': {'accuracy_grader': 0.5, 'relevance_grader': 0.3, 'helpfulness_grader': 0.2}, 'component_scores': {'accuracy_grader': 0.0, 'relevance_grader': 0.08333333333333333, 'helpfulness_grader': 0.0}}, score=0.024999999999999998)}, {'weighted_result': GraderScore(name='weighted_average', reason='Weighted average score calculated from 3 evaluators. accuracy_grader: 1.0 (weight: 0.5); relevance_grader: 0.7037037037037037 (weight: 0.3); helpfulness_grader: 1.0 (weight: 0.2)', metadata={'weights': {'accuracy_grader': 0.5, 'relevance_grader': 0.3, 'helpfulness_grader': 0.2}, 'component_scores': {'accuracy_grader': 1.0, 'relevance_grader': 0.7037037037037037, 'helpfulness_grader': 1.0}}, score=0.9111111111111112)}]\n",
      "  Overall Accuracy: 100.00%\n",
      "  Weighted Scores per Sample:\n",
      "    Sample 1: 0.75 - Weighted average score calculated from 3 evaluators. accuracy_grader: 1.0 (weight: 0.5); relevance_grader: 0.16666666666666666 (weight: 0.3); helpfulness_grader: 1.0 (weight: 0.2)\n",
      "    Sample 2: 0.02 - Weighted average score calculated from 3 evaluators. accuracy_grader: 0.0 (weight: 0.5); relevance_grader: 0.08333333333333333 (weight: 0.3); helpfulness_grader: 0.0 (weight: 0.2)\n",
      "    Sample 3: 0.91 - Weighted average score calculated from 3 evaluators. accuracy_grader: 1.0 (weight: 0.5); relevance_grader: 0.7037037037037037 (weight: 0.3); helpfulness_grader: 1.0 (weight: 0.2)\n"
     ]
    }
   ],
   "source": [
    "# Create analyzers\n",
    "accuracy_analyzer = AccuracyAnalyzer()\n",
    "\n",
    "# Prepare weights for weighted average\n",
    "weights = {\n",
    "    \"accuracy_grader\": 0.5,\n",
    "    \"relevance_grader\": 0.3,\n",
    "    \"helpfulness_grader\": 0.2\n",
    "}\n",
    "\n",
    "weighted_analyzer = WeightedAverageAnalyzer(weights=weights)\n",
    "\n",
    "# Run analysis\n",
    "accuracy_analysis = accuracy_analyzer.analyze(data, results, target_grader=\"helpfulness_grader\")\n",
    "weighted_analysis = weighted_analyzer.analyze(data, results)\n",
    "\n",
    "print(\"Analysis results:\")\n",
    "print(f\"  Accuracy Analysis: {accuracy_analysis}\")\n",
    "print(f\"  Weighted Average Analysis: {weighted_analysis}\")\n",
    "\n",
    "if hasattr(accuracy_analysis, 'accuracy'):\n",
    "    print(f\"  Overall Accuracy: {accuracy_analysis.accuracy:.2%}\")\n",
    "    \n",
    "\n",
    "if hasattr(weighted_analysis, 'weighted_results'):\n",
    "    print(\"  Weighted Scores per Sample:\")\n",
    "    for i, result_dict in enumerate(weighted_analysis.weighted_results):\n",
    "        weighted_result = result_dict[\"weighted_result\"]\n",
    "        print(f\"    Sample {i+1}: {weighted_result.score:.2f} - {weighted_result.reason}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Custom LLMGrader Explanation\n",
    "\n",
    "The custom LLMGrader we implemented shows how to build a fully custom grader that uses LLMs for evaluation:\n",
    "\n",
    "1. **Model Integration**: We used OpenAIChatModel to connect to the OpenAI API\n",
    "2. **Template-based Prompting**: We created a ChatTemplate with specific instructions for the LLM\n",
    "3. **Structured Output**: We instructed the LLM to respond in a specific JSON format for easy parsing\n",
    "4. **Error Handling**: We included error handling to return a default score if the LLM call fails\n",
    "\n",
    "Key features of our custom implementation:\n",
    "\n",
    "```python\n",
    "# The grader uses a specific template to ensure consistent evaluation\n",
    "self.template = ChatTemplate([\n",
    "    ChatMessage(role=\"system\", content=\"Evaluation instructions...\"),\n",
    "    ChatMessage(role=\"user\", content=\"Question: {{query}}\\nAnswer: {{answer}}\")\n",
    "])\n",
    "\n",
    "# The LLM is instructed to return a specific JSON format\n",
    "# {\"score\": 0.8, \"reason\": \"Explanation of the score\"}\n",
    "```\n",
    "\n",
    "This approach gives you full control over:\n",
    "- The LLM model being used\n",
    "- The prompt template and evaluation criteria\n",
    "- How the LLM response is parsed and converted to a GraderResult\n",
    "- Error handling and fallback behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated the complete workflow of RM-Gallery:\n",
    "\n",
    "1. **Data Input**: We prepared sample data with queries, answers, and expected values.\n",
    "2. **Evaluation Execution**: We configured and ran multiple graders concurrently using GradingRunner.\n",
    "3. **Result Organization**: Results were organized by sample, with each grader's output captured.\n",
    "4. **Analysis**: We used analyzers to compute overall metrics and weighted averages.\n",
    "5. **Custom LLMGrader**: We showed how to implement a fully custom LLM-based grader.\n",
    "\n",
    "This modular approach allows for flexible evaluation pipelines where different graders can be combined and analyzed in various ways. The combination of rule-based graders (like AccuracyGrader) and LLM-based graders (like our custom HelpfulnessGrader) provides both precision and nuanced evaluation capabilities."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rm_gallery_310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
